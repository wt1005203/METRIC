{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfig_dims = figure_size_setting(700)\\nfig_dims = [fig_dims[0], fig_dims[1]*1]\\nfig, axes = plt.subplots(1, 1, figsize=fig_dims, sharex=True)\\n\\naxes.set_xlabel(\"\")\\naxes.set_title(\\'\\')\\n\\nfig.tight_layout()\\n#fig.subplots_adjust(left=.1, bottom=.12, right=.97, top=.93, hspace=0.1)\\n#fig.savefig(\"./figures/XXX.png\", dpi=300)\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from scipy.stats import sem\n",
    "import os\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import scipy\n",
    "from scipy.integrate import odeint\n",
    "import scipy.integrate as integ\n",
    "\n",
    "#%% Plot Tong's default setting\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 15\n",
    "BIGGER_SIZE = 15\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE, family='sans-serif', serif='Arial')          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rc('text')\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "my_locator = MaxNLocator(6)\n",
    "\n",
    "color_list = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "def figure_size_setting(WIDTH):\n",
    "    #WIDTH = 700.0  # the number latex spits out\n",
    "    FACTOR = 0.8  # the fraction of the width you'd like the figure to occupy\n",
    "    fig_width_pt  = WIDTH * FACTOR\n",
    "    inches_per_pt = 1.0 / 72.27\n",
    "    golden_ratio  = (np.sqrt(5) - 1.0) / 2.0  # because it looks good\n",
    "    fig_width_in  = fig_width_pt * inches_per_pt  # figure width in inches\n",
    "    fig_height_in = fig_width_in * golden_ratio   # figure height in inches\n",
    "    fig_dims    = [fig_width_in, fig_height_in] # fig dims as a list\n",
    "    return fig_dims\n",
    "\n",
    "'''\n",
    "fig_dims = figure_size_setting(700)\n",
    "fig_dims = [fig_dims[0], fig_dims[1]*1]\n",
    "fig, axes = plt.subplots(1, 1, figsize=fig_dims, sharex=True)\n",
    "\n",
    "axes.set_xlabel(\"\")\n",
    "axes.set_title('')\n",
    "\n",
    "fig.tight_layout()\n",
    "#fig.subplots_adjust(left=.1, bottom=.12, right=.97, top=.93, hspace=0.1)\n",
    "#fig.savefig(\"./figures/XXX.png\", dpi=300)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data (dietary information and metagenome) from Dan Knights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nutrition + microbiome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166, 566)\n"
     ]
    }
   ],
   "source": [
    "df_microbiome_ori = pd.read_csv(\"../Data_from_Dan_Knights/microbiome/processed_sample/taxonomy_counts_s.txt\", delimiter='\\t', index_col=0);\n",
    "#df_diet_ori = pd.read_csv(\"../Data_from_Dan_Knights/diet/processed_nutr/nutr_65.txt\", delimiter='\\t', index_col=0);\n",
    "#df_diet_ori = pd.read_csv(\"../Data_from_Dan_Knights/diet/nutrition_totals.txt\", delimiter='\\t', index_col=0).iloc[:,4:-1].transpose()\n",
    "df_diet_ori = pd.read_csv(\"../Data_from_Dan_Knights/diet/processed_food/dhydrt.txt\", delimiter='\\t', index_col=0).iloc[:,:-1];\n",
    "df_food_taxanomy = pd.read_csv(\"../Data_from_Dan_Knights/diet/processed_food/dhydrt.txt\", delimiter='\\t', index_col=0).iloc[:,-1];\n",
    "\n",
    "df_diet_temp = df_diet_ori.copy()\n",
    "df_diet_temp['food_taxa'] = df_food_taxanomy.apply(lambda x: (x.split(\";L3_\")[1]).split(\";\")[0])\n",
    "df_diet_temp = df_diet_temp.groupby('food_taxa').sum().iloc[1:,:]\n",
    "#df_diet_temp = df_diet_temp.loc[(df_diet_temp>0).mean(1) > 0.05, :]\n",
    "df_diet_ori = df_diet_temp.copy()\n",
    "print(df_diet_temp.shape)\n",
    "\n",
    "#df_diet_ori = df_diet_ori.loc[(df_diet_ori>0).mean(1) > 0.05, :]\n",
    "#print(df_diet_ori.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### select common subjects\n",
    "i_intersected_subjects = np.intersect1d(df_diet_ori.columns, df_microbiome_ori.columns);\n",
    "df_microbiome = df_microbiome_ori.loc[:, i_intersected_subjects];\n",
    "df_diet = df_diet_ori.loc[:, i_intersected_subjects];\n",
    "\n",
    "#### select species only\n",
    "i_valid_species = np.where(list(map(lambda x: \"s__\" in x, df_microbiome.index)))[0]\n",
    "df_microbiome = df_microbiome.iloc[i_valid_species]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(475, 166) (475, 191)\n"
     ]
    }
   ],
   "source": [
    "from skbio.stats.composition import clr\n",
    "pseudo_count = df_diet.transpose()[df_diet.transpose()>0].min().min() / 10\n",
    "#diet_comp_df = pd.DataFrame(data=np.transpose(clr(df_diet.transpose() + pseudo_count)), \n",
    "#                             index=df_diet.index, columns=df_diet.columns)\n",
    "diet_comp_df = np.log(df_diet.transpose() + pseudo_count).transpose()\n",
    "pseudo_count = df_microbiome.transpose()[df_microbiome.transpose()>0].min().min() / 10\n",
    "micro_comp_df = pd.DataFrame(data=np.transpose(clr(df_microbiome.transpose() + pseudo_count)), \n",
    "                             index=df_microbiome.index, columns=df_microbiome.columns)\n",
    "\n",
    "diet_comp_df = diet_comp_df.transpose()\n",
    "micro_comp_df = micro_comp_df.transpose()\n",
    "\n",
    "print(diet_comp_df.shape, micro_comp_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "for noise_level in [0.0, 0.5, 1.0, 1.5, 2.0]:  \n",
    "    noises = np.random.normal(loc=0.0, scale=np.log10(10**noise_level), size=diet_comp_df.values.shape)\n",
    "    X = np.concatenate([micro_comp_df.values, diet_comp_df.values+noises], axis=1)\n",
    "    y = diet_comp_df.values\n",
    "    for i, random_state in enumerate([42,43,44,45,46]):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "        path_prefix = \"./noise_level_\"+str(noise_level)+\"/microbiome_food_split\"+str(i+1)+\"/processed_data/\"\n",
    "        if os.path.exists(\"/\".join(path_prefix.split(\"/\")[:-3]))==False:\n",
    "            os.mkdir(\"/\".join(path_prefix.split(\"/\")[:-3])) \n",
    "        if os.path.exists(\"/\".join(path_prefix.split(\"/\")[:-2]))==False:\n",
    "            os.mkdir(\"/\".join(path_prefix.split(\"/\")[:-2])) \n",
    "        if os.path.exists(path_prefix)==False:\n",
    "            os.mkdir(path_prefix)\n",
    "        np.savetxt(path_prefix + \"X_train.csv\", X_train, delimiter=',')\n",
    "        np.savetxt(path_prefix + \"y_train.csv\", y_train, delimiter=',')\n",
    "        np.savetxt(path_prefix + \"X_test.csv\", X_test, delimiter=',')\n",
    "        np.savetxt(path_prefix + \"y_test.csv\", y_test, delimiter=',')\n",
    "        np.savetxt(path_prefix + \"compound_names.csv\", diet_comp_df.columns, delimiter='\\t', fmt = '%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative train-test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor noise_level in [0.5, 1.0, 2.0]:  \\n    from sklearn.model_selection import KFold\\n    kf = KFold(n_splits=5, random_state=42)\\n    noises = np.random.normal(loc=0.0, scale=np.log10(10**noise_level), size=diet_comp_df.values.shape)\\n    X = np.concatenate([micro_comp_df.values, diet_comp_df.values+noises], axis=1)\\n    y = diet_comp_df.values\\n\\n    for i, (train_index, test_index) in enumerate(kf.split(X)):\\n        X_train, X_test, y_train, y_test = X[train_index,:], X[test_index,:], y[train_index,:], y[test_index,:]\\n\\n        path_prefix = \"./noise_level_\"+str(noise_level)+\"/microbiome_food_split\"+str(i+1)+\"/processed_data/\"\\n        if os.path.exists(\"/\".join(path_prefix.split(\"/\")[:-3]))==False:\\n            os.mkdir(\"/\".join(path_prefix.split(\"/\")[:-3])) \\n        if os.path.exists(\"/\".join(path_prefix.split(\"/\")[:-2]))==False:\\n            os.mkdir(\"/\".join(path_prefix.split(\"/\")[:-2])) \\n        if os.path.exists(path_prefix)==False:\\n            os.mkdir(path_prefix) \\n        np.savetxt(path_prefix + \"X_train.csv\", X_train, delimiter=\\',\\')\\n        np.savetxt(path_prefix + \"y_train.csv\", y_train, delimiter=\\',\\')\\n        np.savetxt(path_prefix + \"X_test.csv\", X_test, delimiter=\\',\\')\\n        np.savetxt(path_prefix + \"y_test.csv\", y_test, delimiter=\\',\\')\\n        np.savetxt(path_prefix + \"compound_names.csv\", diet_comp_df.columns, delimiter=\\'\\t\\', fmt = \\'%s\\')\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for noise_level in [0.5, 1.0, 2.0]:  \n",
    "    from sklearn.model_selection import KFold\n",
    "    kf = KFold(n_splits=5, random_state=42)\n",
    "    noises = np.random.normal(loc=0.0, scale=np.log10(10**noise_level), size=diet_comp_df.values.shape)\n",
    "    X = np.concatenate([micro_comp_df.values, diet_comp_df.values+noises], axis=1)\n",
    "    y = diet_comp_df.values\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test, y_train, y_test = X[train_index,:], X[test_index,:], y[train_index,:], y[test_index,:]\n",
    "\n",
    "        path_prefix = \"./noise_level_\"+str(noise_level)+\"/microbiome_food_split\"+str(i+1)+\"/processed_data/\"\n",
    "        if os.path.exists(\"/\".join(path_prefix.split(\"/\")[:-3]))==False:\n",
    "            os.mkdir(\"/\".join(path_prefix.split(\"/\")[:-3])) \n",
    "        if os.path.exists(\"/\".join(path_prefix.split(\"/\")[:-2]))==False:\n",
    "            os.mkdir(\"/\".join(path_prefix.split(\"/\")[:-2])) \n",
    "        if os.path.exists(path_prefix)==False:\n",
    "            os.mkdir(path_prefix) \n",
    "        np.savetxt(path_prefix + \"X_train.csv\", X_train, delimiter=',')\n",
    "        np.savetxt(path_prefix + \"y_train.csv\", y_train, delimiter=',')\n",
    "        np.savetxt(path_prefix + \"X_test.csv\", X_test, delimiter=',')\n",
    "        np.savetxt(path_prefix + \"y_test.csv\", y_test, delimiter=',')\n",
    "        np.savetxt(path_prefix + \"compound_names.csv\", diet_comp_df.columns, delimiter='\\t', fmt = '%s')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Bacon_salt_pork', 'Bars', 'Beef_NS',\n",
       "       'Beef_oxtails_neckbones_short_ribs_head',\n",
       "       'Beef_roasts_stew_meat_corned_beef_beef_brisket_sandwich_steaks',\n",
       "       'Beef_steak', 'Beers_and_ales', 'Berries', 'Biscuits', 'Breadrolls_NFS',\n",
       "       ...\n",
       "       'White_breads_rolls', 'White_potatoes_NFS',\n",
       "       'White_potatoes_baked_and_boiled', 'White_potatoes_chips_and_sticks',\n",
       "       'White_potatoes_fried', 'White_potatoes_mashed_stuffed_puffs',\n",
       "       'Whole_wheat_breads_rolls', 'Wines', 'Yogurt',\n",
       "       'meatpoultryfish_in_gravy'],\n",
       "      dtype='object', name='food_taxa', length=166)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diet_comp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nteract": {
   "version": "0.28.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
